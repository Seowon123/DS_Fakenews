{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "# ================== 1. 데이터 로드 및 전처리 ==================\n",
    "# JSON 파일 읽기\n",
    "def read_json(folder_paths):\n",
    "    data = []\n",
    "    for folder_path in folder_paths:\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.json'):\n",
    "                with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "                    json_data = json.load(file)\n",
    "                    data.append({\n",
    "                        \"newsTitle\": json_data[\"sourceDataInfo\"][\"newsTitle\"],\n",
    "                        \"newsContent\": json_data[\"sourceDataInfo\"][\"newsContent\"],\n",
    "                        \"clickbaitClass\": json_data[\"sourceDataInfo\"][\"useType\"]\n",
    "                    })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text, stopwords):\n",
    "    # 백슬래시 제거\n",
    "    text = text.replace(\"\\\\\", \"\")\n",
    "    # 중복된 공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return ' '.join(word for word in text.split() if word not in stopwords)\n",
    "\n",
    "\n",
    "# 불용어 로드\n",
    "def load_stopwords(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "# 피처 생성\n",
    "def add_features(df):\n",
    "    df['titleLength'] = df['cleanedTitle'].apply(len)\n",
    "    df['contentLength'] = df['cleanedContent'].apply(len)\n",
    "    df['title_content_ratio'] = df['titleLength'] / (df['contentLength'] + 1e-6)\n",
    "    df['contentLexicalDiversity'] = df['cleanedContent'].apply(\n",
    "        lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0\n",
    "    )\n",
    "    df['specialCharRatio'] = df['combined_text'].apply(lambda x: special_char_ratio_features(x)['special_char_ratio'])\n",
    "    return df\n",
    "\n",
    "# TF-IDF 및 수치형 피처 결합\n",
    "def process_features(df, tfidf_vectorizer=None, scaler=None, train=True):\n",
    "    if train:\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        scaler = RobustScaler()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleanedContent'])\n",
    "        scaled_features = scaler.fit_transform(df[['contentLength', 'title_content_ratio', 'contentLexicalDiversity','specialCharRatio']])\n",
    "    else:\n",
    "        tfidf_matrix = tfidf_vectorizer.transform(df['cleanedContent'])\n",
    "        scaled_features = scaler.transform(df[['contentLength', 'title_content_ratio', 'contentLexicalDiversity','specialCharRatio']])\n",
    "    combined_features = hstack((tfidf_matrix, scaled_features))\n",
    "    return combined_features, tfidf_vectorizer, scaler\n",
    "\n",
    "def special_char_ratio_features(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return {'special_char_ratio': 0}\n",
    "    # 특수문자 비율\n",
    "    special_chars = re.findall(r'[^\\w\\s]', text)  # 특수문자\n",
    "    special_char_ratio = len(special_chars) / len(text)\n",
    "    return {\n",
    "        'special_char_ratio': special_char_ratio\n",
    "    }\n",
    "\n",
    "\n",
    "# ================== 2. 모델 학습 및 평가 ==================\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(f\"\\n{model.__class__.__name__} Evaluation\")\n",
    "    print('Accuracy:\\t', accuracy_score(y_test, y_pred))\n",
    "    print('Recall:\\t\\t', recall_score(y_test, y_pred))\n",
    "    print('Precision:\\t', precision_score(y_test, y_pred))\n",
    "    print('AUC:\\t\\t', roc_auc)\n",
    "    print('F1 Score:\\t', f1_score(y_test, y_pred))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc(fpr, tpr):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.title(f'ROC Curve - {model.__class__.__name__}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "def grid_search_tuning(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best Params: {grid_search.best_params_}, Best F1: {grid_search.best_score_:.4f}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Validation 평가 함수\n",
    "def evaluate_on_validation(model, X_validation, y_validation):\n",
    "    y_val_pred = model.predict(X_validation)\n",
    "    y_val_proba = model.predict_proba(X_validation)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_validation, y_val_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(f\"Validation Set Evaluation - {model.__class__.__name__}\")\n",
    "    print('Accuracy:\\t', accuracy_score(y_test, y_val_pred))\n",
    "    print('Recall:\\t\\t', recall_score(y_test, y_val_pred))\n",
    "    print('Precision:\\t', precision_score(y_test, y_val_pred))\n",
    "    print('AUC:\\t\\t', roc_auc)\n",
    "    print('F1 Score:\\t', f1_score(y_test, y_val_pred))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc(fpr, tpr):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.title(f'Validation ROC Curve - {model.__class__.__name__}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================== 3. 테스트 데이터 적용 ======================\n",
    "def predict_new_data(test_data_path, model, tfidf_vectorizer, scaler):\n",
    "    with open(test_data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        test_data = pd.DataFrame(json.load(file))\n",
    "    \n",
    "    test_data['cleanedTitle'] = test_data['title'].apply(lambda x: preprocess_text(x, stopwords))\n",
    "    test_data['cleanedContent'] = test_data['content'].apply(lambda x: preprocess_text(x, stopwords))\n",
    "    test_data[\"combined_text\"] = test_data[\"title\"] + \" \" + test_data[\"content\"]\n",
    "    test_data = add_features(test_data)\n",
    "\n",
    "    X_test, _, _ = process_features(test_data, scaler, tfidf_vectorizer,train=False)\n",
    "    predictions = model.predict_proba(X_test)\n",
    "\n",
    "    for title, proba in zip(test_data['title'], predictions):\n",
    "        label = \"Clickbait\" if proba[1] > proba[0] else \"Normal\"\n",
    "        print(f\"제목: {title}\")\n",
    "        print(f\"예측 결과: {label}, Clickbait 확률: {proba[1]:.2f}, Normal 확률: {proba[0]:.2f}\\n\")\n",
    "\n",
    "\n",
    "# ================== 4. 주요 실행 ==================\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 로드 및 전처리\n",
    "    folders = [\n",
    "        \"./Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_SO\",\n",
    "        \"./Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_SO\",\n",
    "        \"./Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_SO\"\n",
    "    ]\n",
    "    stopwords = load_stopwords(\"stopwords-ko.txt\")\n",
    "    df = read_json(folders)\n",
    "    df['cleanedTitle'] = df['newsTitle'].apply(lambda x: preprocess_text(x, stopwords))\n",
    "    df['cleanedContent'] = df['newsContent'].apply(lambda x: preprocess_text(x, stopwords))\n",
    "    df[\"combined_text\"] = df[\"newsTitle\"] + \" \" + df[\"newsContent\"]\n",
    "    df = add_features(df)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 데이터 준비\n",
    "    X, tfidf_vectorizer, scaler = process_features(df, train=True)\n",
    "    y = df['clickbaitClass']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Validation 데이터 로드 및 전처리\n",
    "    validation_folders = [\n",
    "        \"./Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_SO\",\n",
    "        \"./Validation/02.라벨링데이터/VL_Part1_Clickbait_Direct_SO\",\n",
    "        \"./Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_SO\"\n",
    "    ]\n",
    "    df_validation = read_json(validation_folders)\n",
    "    df_validation['cleanedTitle'] = df_validation['newsTitle'].apply(lambda x: preprocess_text(x, stopwords))\n",
    "    df_validation['cleanedContent'] = df_validation['newsContent'].apply(lambda x: preprocess_text(x, stopwords))\n",
    "    df_validation[\"combined_text\"] = df_validation[\"newsTitle\"] + \" \" + df_validation[\"newsContent\"]\n",
    "    df_validation = add_features(df_validation)\n",
    "    X_validation, _, _ = process_features(df_validation, tfidf_vectorizer, scaler, train=False)\n",
    "    y_validation = df_validation['clickbaitClass']\n",
    "\n",
    "    # 하이퍼파라미터 튜닝 및 최적 모델 선정\n",
    "    rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "    best_rf = grid_search_tuning(RandomForestClassifier(random_state=42), rf_param_grid, X_train, y_train)\n",
    "\n",
    "    lr_param_grid = {'C': [1, 10], 'solver': ['lbfgs', 'saga']}\n",
    "    best_lr = grid_search_tuning(LogisticRegression(max_iter=1000), lr_param_grid, X_train, y_train)\n",
    "\n",
    "    # 최적 모델 평가 및 시각화\n",
    "    evaluate_model(best_rf, X_test, y_test)\n",
    "    evaluate_model(best_lr, X_test, y_test)\n",
    "\n",
    "    # Validation 데이터 평가\n",
    "    evaluate_on_validation(best_rf, X_validation, y_validation)\n",
    "    evaluate_on_validation(best_lr, X_validation, y_validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\angel\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RobustScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"방송통신심의위원회는 JTBC 예능 프로그램 '이혼숙려캠프 새로고침'이 지나치게 적나라한 내용을 방영했다고 지적하며 방송 관계자 의견을 듣기로 했다. 방심위는 17일 서울 양천구 목동 방송회관에서 전체 회의를 열고 음주 상태에서 아내에게 폭언하는 남편의 행동이나 선정적인 내용을 방영한 이혼숙려캠프에 대해 관계자 의견진술을 의결했다. 방심위는 이혼숙려캠프가 객관적 근거 없이 남성의 성욕 등에 일반화해 설명하는 성 역할에 대한 고정관념을 조장할 우려가 있다고 판단했다. 김정수 방심위원은 ‘이혼 사유가 내밀한 문제이긴 집안에서 나눈 대화가 여과 없이 노출되고 있다’며 ‘제재받더라도 시청률이 더 중요하다는 제작진의 안일한 인식이 문제’라고 했다. 류희림 방심위원장은 ‘리얼리티 프로그램에서 너무 적나라한 표현이나 사적인 내용이 나오고 성관계 문제까지 나온다’며 ‘아무리 청소년 이용 불가(19금)라고 해도 지나친 측면이 있다’고 했다. 방심위는 비속어나 차별적 표현을 남발한 지상파 3사 예능에 대해서도 관계자 의견진술을 의결했다. 의결진술 대상은 출연진이 ‘어우씨’, ‘죽여버려’ 등이라고 이를 자막으로 표기한 SBS '런닝맨', ‘지?하네’라는 발언을 묵음 처리해 내보낸 MBC '놀면 뭐 하니'다. '세기가 주목할 요단강 매치'라는 표현이나, ‘숏 다리가 쓸모가 다 있네’라는 말과 자막을 내보낸 KBS '1박2일'이다.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m제목: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m예측 결과: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Clickbait 확률: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproba[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Normal 확률: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproba[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mpredict_new_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model_test_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_vectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m, in \u001b[0;36mpredict_new_data\u001b[1;34m(test_data_path, model, tfidf_vectorizer, scaler)\u001b[0m\n\u001b[0;32m      8\u001b[0m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m test_data \u001b[38;5;241m=\u001b[39m add_features(test_data)\n\u001b[1;32m---> 11\u001b[0m X_test, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_vectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title, proba \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m], predictions):\n",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m, in \u001b[0;36mprocess_features\u001b[1;34m(df, tfidf_vectorizer, scaler, train)\u001b[0m\n\u001b[0;32m     63\u001b[0m     scaled_features \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontentLength\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_content_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontentLexicalDiversity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecialCharRatio\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleanedContent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     scaled_features \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontentLength\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_content_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontentLexicalDiversity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecialCharRatio\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     67\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m hstack((tfidf_matrix, scaled_features))\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1658\u001b[0m, in \u001b[0;36mRobustScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Center and scale the data.\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \n\u001b[0;32m   1647\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;124;03m    Transformed array.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1657\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1658\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_scaling:\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1031\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1031\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   1033\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"방송통신심의위원회는 JTBC 예능 프로그램 '이혼숙려캠프 새로고침'이 지나치게 적나라한 내용을 방영했다고 지적하며 방송 관계자 의견을 듣기로 했다. 방심위는 17일 서울 양천구 목동 방송회관에서 전체 회의를 열고 음주 상태에서 아내에게 폭언하는 남편의 행동이나 선정적인 내용을 방영한 이혼숙려캠프에 대해 관계자 의견진술을 의결했다. 방심위는 이혼숙려캠프가 객관적 근거 없이 남성의 성욕 등에 일반화해 설명하는 성 역할에 대한 고정관념을 조장할 우려가 있다고 판단했다. 김정수 방심위원은 ‘이혼 사유가 내밀한 문제이긴 집안에서 나눈 대화가 여과 없이 노출되고 있다’며 ‘제재받더라도 시청률이 더 중요하다는 제작진의 안일한 인식이 문제’라고 했다. 류희림 방심위원장은 ‘리얼리티 프로그램에서 너무 적나라한 표현이나 사적인 내용이 나오고 성관계 문제까지 나온다’며 ‘아무리 청소년 이용 불가(19금)라고 해도 지나친 측면이 있다’고 했다. 방심위는 비속어나 차별적 표현을 남발한 지상파 3사 예능에 대해서도 관계자 의견진술을 의결했다. 의결진술 대상은 출연진이 ‘어우씨’, ‘죽여버려’ 등이라고 이를 자막으로 표기한 SBS '런닝맨', ‘지?하네’라는 발언을 묵음 처리해 내보낸 MBC '놀면 뭐 하니'다. '세기가 주목할 요단강 매치'라는 표현이나, ‘숏 다리가 쓸모가 다 있네’라는 말과 자막을 내보낸 KBS '1박2일'이다.\""
     ]
    }
   ],
   "source": [
    "\n",
    "predict_new_data(\"./model_test_data.json\", best_lr, tfidf_vectorizer, scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
