{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ› ï¸ ì–´íœ˜ ë‹¤ì–‘ì„± ê³„ì‚° í•¨ìˆ˜\n",
    "def calculate_lexical_diversity(text):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ì—ì„œ ì–´íœ˜ ë‹¤ì–‘ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    :param text: str, ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "    :return: ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜ (ê³ ìœ  ë‹¨ì–´ ìˆ˜ / ì „ì²´ ë‹¨ì–´ ìˆ˜)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return 0\n",
    "    words = text.split()  # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "    unique_words = set(words)  # ê³ ìœ  ë‹¨ì–´ ì§‘í•©\n",
    "    lexical_diversity = len(unique_words) / len(words) if len(words) > 0 else 0\n",
    "    return lexical_diversity\n",
    "\n",
    "# ğŸ“‚ JSON íŒŒì¼ë¡œë¶€í„° ë°ì´í„° ë¡œë“œ (Clickbait & NonClickbait ê³µí†µ)\n",
    "def load_data_from_json(file_path, label):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    :param file_path: JSON íŒŒì¼ ê²½ë¡œ\n",
    "    :param label: 1 (Clickbait) ë˜ëŠ” 0 (Non-Clickbait)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            df = pd.DataFrame(data)\n",
    "        elif isinstance(data, dict):\n",
    "            df = pd.DataFrame(data.get('articles') or data.get('data') or data)\n",
    "        else:\n",
    "            print(f\"âŒ ì•Œ ìˆ˜ ì—†ëŠ” JSON êµ¬ì¡°ì…ë‹ˆë‹¤: {type(data)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if 'sourceDataInfo' in df.columns:\n",
    "            df['title'] = df['sourceDataInfo'].apply(lambda x: x.get('newsTitle', '') if isinstance(x, dict) else '')\n",
    "            df['content'] = df['sourceDataInfo'].apply(lambda x: x.get('newsContent', '') if isinstance(x, dict) else '')\n",
    "\n",
    "        df['label'] = label  # Clickbait = 1, NonClickbait = 0\n",
    "        return df[['title', 'content', 'label']]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ğŸ“‚ ë°ì´í„° ê²½ë¡œ ì„¤ì • (Clickbait & NonClickbait ë‹¨ì¼ JSON íŒŒì¼)\n",
    "clickbait_path = r'C:\\Ryuna\\ClickBait Detecting related data\\Training\\Labeling\\TL_Part1_Clickbait_Merged_SO\\merged_clickbait.json'\n",
    "nonclickbait_path = r'C:\\Ryuna\\ClickBait Detecting related data\\Training\\Labeling\\TL_Part1_NonClickbait_Merged_SO\\merged_file.json'\n",
    "\n",
    "\n",
    "# ğŸ› ï¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "print(\"ğŸ“‚ Clickbait ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\")\n",
    "clickbait_df = load_data_from_json(clickbait_path, label=1)  # Clickbait ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "print(\"ğŸ“‚ Non-Clickbait ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\")\n",
    "nonclickbait_df = load_data_from_json(nonclickbait_path, label=0)  # Non-Clickbait ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "# ğŸ› ï¸ ë°ì´í„° ë³‘í•©\n",
    "df = pd.concat([clickbait_df, nonclickbait_df], ignore_index=True)\n",
    "print(f\"ğŸ“š ë°ì´í„° ë³‘í•© ì™„ë£Œ. ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "\n",
    "# ğŸ› ï¸ ì–´íœ˜ ë‹¤ì–‘ì„± ë° ì¶”ê°€ í”¼ì²˜ ê³„ì‚°\n",
    "print(\"ğŸ“Š ì–´íœ˜ ë‹¤ì–‘ì„± ë° ì¶”ê°€ í”¼ì²˜ ê³„ì‚° ì¤‘...\")\n",
    "\n",
    "# Title Lexical Diversity\n",
    "df['title_lexical_diversity'] = df['title'].apply(calculate_lexical_diversity)\n",
    "\n",
    "# Content Lexical Diversity\n",
    "df['content_lexical_diversity'] = df['content'].apply(calculate_lexical_diversity)\n",
    "\n",
    "# Combined Lexical Diversity (Title + Content)\n",
    "df['combined_text'] = df['title'] + ' ' + df['content']\n",
    "df['combined_lexical_diversity'] = df['combined_text'].apply(calculate_lexical_diversity)\n",
    "\n",
    "# Title Length (ë‹¨ì–´ ìˆ˜)\n",
    "df['title_length'] = df['title'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "\n",
    "# Content Length (ë‹¨ì–´ ìˆ˜)\n",
    "df['content_length'] = df['content'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "\n",
    "# ğŸ”¥ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì—´ ì‚­ì œ\n",
    "df = df[['title_lexical_diversity', 'content_lexical_diversity', 'combined_lexical_diversity', \n",
    "         'title_length', 'content_length', 'label']]\n",
    "\n",
    "# ğŸ“˜ ë°ì´í„° ì €ì¥\n",
    "output_path = r'D:\\ë¨¸ì‹ ëŸ¬ë‹\\csv_files\\lexical_diversity_results_combined_naivebayes ver_1215.csv'  # ê²½ë¡œ ìˆ˜ì •\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"âœ… ë¶„ì„ ê²°ê³¼ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
